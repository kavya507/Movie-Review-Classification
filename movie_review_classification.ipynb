{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', header = None)\n",
    "data.columns=['Sentiment', 'Review']\n",
    "data['Sentiment'] = data['Sentiment'].replace(-1, 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pre-trained BERT model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the input format for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    for review, label in zip(data['Review'], data['Sentiment']):\n",
    "        # Encode the review and add the special tokens\n",
    "        encoded = tokenizer.encode_plus(review, add_special_tokens=True, max_length=512, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        # Add the encoded review, attention mask, and label to the lists\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "        labels.append(label)\n",
    "    # Convert the lists to tensors\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    # Return tensors\n",
    "    return input_ids, attention_masks, labels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 4\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_masks, train_labels = encode_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, test_masks, test_labels = encode_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move the model to the device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "loss tensor(0.1804, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-0.7116,  0.8834],\n",
      "        [-0.8951,  0.7527]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1925, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.1550,  0.9251],\n",
      "        [ 0.1786, -1.0041]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.2919, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.7305, -0.6198],\n",
      "        [ 0.1988, -0.6599]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1863, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.2014,  0.6678],\n",
      "        [ 0.5991, -0.7577]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.2466, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-0.9266,  0.5439],\n",
      "        [-0.6145,  0.4892]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.2982, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.1461, -0.5785],\n",
      "        [-0.9429,  0.5575]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.2692, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.2746, -0.9984],\n",
      "        [-0.7728,  0.3104]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1798, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-0.9312,  0.5158],\n",
      "        [-1.2392,  0.5935]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1403, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.0827,  0.8434],\n",
      "        [ 0.7232, -1.1378]], grad_fn=<AddmmBackward0>)\n",
      "Batch train_loss 0.22058011260297564\n",
      "Batch test_loss 1.0\n",
      "Epoch 1, Train Loss: 0.2206, Train Accuracy: 1.0000\n",
      "Hello\n",
      "loss tensor(0.2631, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.1105,  0.4456],\n",
      "        [-0.5320,  0.3903]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1661, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.0941,  0.8117],\n",
      "        [-1.0348,  0.5094]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.5818, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-0.2385, -0.8018],\n",
      "        [-1.2039,  0.6199]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.2641, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.1241, -0.8699],\n",
      "        [ 0.4923, -0.9444]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1532, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.2264,  0.7367],\n",
      "        [ 0.7220, -0.9317]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.2652, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.1917, -0.9217],\n",
      "        [ 0.3593, -0.9157]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.2055, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.4807, -0.7187],\n",
      "        [-1.2037,  0.6350]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.0862, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.5142,  1.0250],\n",
      "        [-1.4398,  0.8506]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.2205, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.2720,  0.9745],\n",
      "        [ 0.4884, -0.4140]], grad_fn=<AddmmBackward0>)\n",
      "Batch train_loss 0.24507656445105871\n",
      "Batch test_loss 0.9444444444444444\n",
      "Epoch 2, Train Loss: 0.2451, Train Accuracy: 0.9444\n",
      "Hello\n",
      "loss tensor(0.1093, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.3251,  0.9398],\n",
      "        [-1.1802,  0.8818]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1592, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.3379, -1.0259],\n",
      "        [-1.2208,  1.1331]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.0959, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.2309,  0.9855],\n",
      "        [-1.5007,  0.8816]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1036, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 1.0023, -1.1459],\n",
      "        [-1.4050,  0.8817]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1128, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.2973,  0.9434],\n",
      "        [-1.3141,  0.7064]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.2422, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.5492, -0.8403],\n",
      "        [ 0.3629, -0.8426]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1489, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.6829,  1.1421],\n",
      "        [ 0.5026, -0.8009]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1247, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.4688,  1.0935],\n",
      "        [ 0.4972, -1.1568]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1175, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.6204, -1.1467],\n",
      "        [-1.2912,  1.2290]], grad_fn=<AddmmBackward0>)\n",
      "Batch train_loss 0.1349059897992346\n",
      "Batch test_loss 1.0\n",
      "Epoch 3, Train Loss: 0.1349, Train Accuracy: 1.0000\n",
      "Hello\n",
      "loss tensor(0.0915, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.5528,  1.1161],\n",
      "        [ 0.7518, -1.3432]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1273, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.4666,  0.7822],\n",
      "        [ 0.9388, -0.8516]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1101, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.5367, -1.1871],\n",
      "        [-1.4793,  1.3740]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.1109, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.7293,  0.9694],\n",
      "        [ 1.0956, -0.6783]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.0947, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 0.7911, -1.2030],\n",
      "        [-1.6995,  1.0531]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.0574, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.3777,  1.3158],\n",
      "        [-1.5426,  1.4435]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.0524, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.5705,  1.2633],\n",
      "        [-1.5423,  1.4770]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.0666, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[-1.6084,  1.5004],\n",
      "        [ 1.0514, -1.3175]], grad_fn=<AddmmBackward0>)\n",
      "Hello\n",
      "loss tensor(0.0892, grad_fn=<NllLossBackward0>)\n",
      "logits tensor([[ 1.0668, -1.0576],\n",
      "        [-1.5099,  1.1832]], grad_fn=<AddmmBackward0>)\n",
      "Batch train_loss 0.08890412002801895\n",
      "Batch test_loss 1.0\n",
      "Epoch 4, Train Loss: 0.0889, Train Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Initialize the training loss and accuracy\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    # Loop over the training batches\n",
    "    for batch in train_dataloader:\n",
    "        print(\"Hello\")\n",
    "        # Move the batch to the device\n",
    "        for i in range(len(batch)):\n",
    "            batch[i] = batch[i].to(device)\n",
    "        # Get the input ids, attention masks, and labels\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "        labels = batch[2]\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        # Get the loss and logits\n",
    "        loss = outputs[0]\n",
    "        print(f'loss', loss)\n",
    "        logits = outputs[1]\n",
    "        print(f'logits', logits)\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update the training loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        train_acc += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "    # Compute the average training loss and accuracy\n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_acc = train_acc / len(train_dataloader)\n",
    "    print(f'Batch train_loss', train_loss)\n",
    "    print(f'Batch test_loss', train_acc)\n",
    "    # Print the training results\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3109, Test Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# Initialize the test loss and accuracy\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "# Loop over the test batches\n",
    "for batch in val_dataloader:\n",
    "    # Move the batch to the device\n",
    "    for i in range(len(batch)):\n",
    "            batch[i] = batch[i].to(device)\n",
    "    # Get the input ids, attention masks, and labels\n",
    "    input_ids = batch[0]\n",
    "    attention_mask = batch[1]\n",
    "    labels = batch[2]\n",
    "    # Forward pass with no gradient\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    # Get the loss and logits\n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    # Update the test loss and accuracy\n",
    "    test_loss += loss.item()\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    test_acc += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "# Compute the average test loss and accuracy\n",
    "test_loss = test_loss / len(val_dataloader)\n",
    "test_acc = test_acc / len(val_dataloader)\n",
    "# Print the test results\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84f7fe8bfd97c53db6cb3590e9859b5e22e9a62aa9835e15b36d71b7a5fa40af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
